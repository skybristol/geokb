{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook works through an initial baseline representation of a subset of Pubs Warehouse catalog records for publications and USGS/other gov reports. Initially, we are pulling all USGS Numbered Series reports and all other items (journal articles, etc.) that have at least one contributor listed with an ORCID identifier (meaning we have the ability to link the product to a person and use the information in capacity assessment use cases).\n",
    "\n",
    "Within this process, we also add new representations of people whose ORCID IDs turn up as contributors but where we didn't already turn them up via personnel profiles. This lets us link to those people as authors, editors, or compilers when we build out the product items.\n",
    "\n",
    "In addition to the barebones information on each product (title, derived description, publication year, identifiers, contributor links), we also write any abstracts and tables of contents available in the PW Catalog to the item discussion page.\n",
    "\n",
    "One a baseline is established, we can use a dates written to the [Pubs Warehouse Catalog](https://geokb.wikibase.cloud/wiki/Item:Q54915) source item to determine the parameter (mod_x_days) we need to send to the PW web service for changes since our last processing. That will be a slightly different process we will set up to run on a schedule that will pull in most of the functionality started here but will work out how to deal with changes introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "# import pypandoc\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from urllib.parse import quote\n",
    "from wbmaker import WikibaseConnection\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "geokb = WikibaseConnection('GEOKB_CLOUD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pwid_values(qid, pid):\n",
    "    r = requests.get(f\"https://geokb.wikibase.cloud/w/api.php?action=wbgetclaims&format=json&entity={qid}&property={pid}\").json()\n",
    "    if r['claims']:\n",
    "        return [{'qid': qid, 'pid': pid, 'claim_id': i['id'], 'claim_value': i['mainsnak']['datavalue']['value']} for i in r['claims']['P114']]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "def redo_claim(wb, qid, pid, claim_value, refs):\n",
    "    item = wb.wbi.item.get(qid)\n",
    "    item.claims.remove(pid)\n",
    "    r = item.write(summary=f\"Removed {pid} claim\")\n",
    "\n",
    "    item = wb.wbi.item.get(qid)\n",
    "    item.claims.add(\n",
    "        wb.datatypes.ExternalID(\n",
    "            prop_nr=pid,\n",
    "            value=claim_value,\n",
    "            references=refs\n",
    "        )\n",
    "    )\n",
    "    r = item.write(summary=f\"Added {pid} claim\")\n",
    "\n",
    "    return r.id\n",
    "\n",
    "def entity_data(qid):\n",
    "    return requests.get(f\"https://geokb.wikibase.cloud/wiki/Special:EntityData/{qid}.json\").json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>pid</th>\n",
       "      <th>claim_id</th>\n",
       "      <th>claim_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q144957</td>\n",
       "      <td>P114</td>\n",
       "      <td>Q144957$AFBE17A5-442C-47AF-8C74-DD1FEAB97946</td>\n",
       "      <td>70202469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q144958</td>\n",
       "      <td>P114</td>\n",
       "      <td>Q144958$C05DAE73-14E7-4BF3-ABEA-D7104EF7875F</td>\n",
       "      <td>70196687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q145343</td>\n",
       "      <td>P114</td>\n",
       "      <td>Q145343$2F648780-E3C4-4314-AE5C-FACD374385B8</td>\n",
       "      <td>70217681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q145344</td>\n",
       "      <td>P114</td>\n",
       "      <td>Q145344$E31EA774-C890-4508-A58C-98773C3058C7</td>\n",
       "      <td>70227916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q145374</td>\n",
       "      <td>P114</td>\n",
       "      <td>Q145374$7A88DED2-6F81-49AB-8C80-A236B49D838B</td>\n",
       "      <td>70195145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid   pid                                      claim_id claim_value\n",
       "0  Q144957  P114  Q144957$AFBE17A5-442C-47AF-8C74-DD1FEAB97946    70202469\n",
       "1  Q144958  P114  Q144958$C05DAE73-14E7-4BF3-ABEA-D7104EF7875F    70196687\n",
       "2  Q145343  P114  Q145343$2F648780-E3C4-4314-AE5C-FACD374385B8    70217681\n",
       "3  Q145344  P114  Q145344$E31EA774-C890-4508-A58C-98773C3058C7    70227916\n",
       "4  Q145374  P114  Q145374$7A88DED2-6F81-49AB-8C80-A236B49D838B    70195145"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwid_fixes = pd.read_parquet('./data/pid_fixes.parquet')\n",
    "pwid_fixes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get GeoKB PW Records\n",
    "I have a problem here in that the query is not returning all items via the SPARQL service. Until that gets resolved, I had to run a few queries on PW indexId identifiers and stash the results in files so that I get a complete set of pubs that already have a representation. There is something of a basic safeguard in place in Wikibase that means we can't introduce two items with exactly the same label and classification. Trying to do so, throws an exception from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geokb_pw_ids = geokb.url_sparql_query(\n",
    "    sparql_url=\"https://geokb.wikibase.cloud/query/sparql?query=PREFIX%20wdt%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fprop%2Fdirect%2F%3E%0A%0ASELECT%20%3Fitem%20%3FindexId%0AWHERE%20%7B%0A%20%20%3Fitem%20wdt%3AP114%20%3FindexId%20.%0A%7D\",\n",
    "    output_format=\"dataframe\"\n",
    ")\n",
    "geokb_pw_ids['qid'] = geokb_pw_ids['item'].apply(lambda x: x.split('/')[-1])\n",
    "geokb_pw_ids.drop(columns=\"item\", inplace=True)\n",
    "#geokb_pw_ids.drop_duplicates(inplace=True)\n",
    "\n",
    "geokb_doi_ids = geokb.url_sparql_query(\n",
    "    sparql_url=\"https://geokb.wikibase.cloud/query/sparql?query=PREFIX%20wdt%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fprop%2Fdirect%2F%3E%0A%0ASELECT%20%3Fitem%20%3Fdoi%0AWHERE%20%7B%0A%20%20%3Fitem%20wdt%3AP74%20%3Fdoi%20.%0A%7D\",\n",
    "    output_format=\"dataframe\"\n",
    ")\n",
    "geokb_doi_ids['qid'] = geokb_doi_ids['item'].apply(lambda x: x.split('/')[-1])\n",
    "geokb_doi_ids.drop(columns=\"item\", inplace=True)\n",
    "#geokb_doi_ids.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "geokb_pub_ids = pd.read_csv('./data/geokb_indexid_doi.csv')\n",
    "geokb_pub_ids['qid'] = geokb_pub_ids['item'].apply(lambda x: x.split('/')[-1])\n",
    "geokb_pub_ids.drop(columns=\"item\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_entities = geokb.url_sparql_query(\n",
    "    sparql_url=\"https://geokb.wikibase.cloud/query/sparql?query=PREFIX%20wd%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fentity%2F%3E%0APREFIX%20wdt%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fprop%2Fdirect%2F%3E%0A%0ASELECT%20%3Fitem%20%3FitemLabel%0AWHERE%20%7B%0A%20%20%3Fpub_classes%20wdt%3AP2*%20wd%3AQ6%20.%0A%20%20%3Fitem%20wdt%3AP1%20%3Fpub_classes%20.%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20.%20%7D%0A%7D\",\n",
    "    output_format=\"dataframe\"\n",
    ")\n",
    "\n",
    "report_entities = geokb.url_sparql_query(\n",
    "    sparql_url=\"https://geokb.wikibase.cloud/query/sparql?query=PREFIX%20wd%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fentity%2F%3E%0APREFIX%20wdt%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fprop%2Fdirect%2F%3E%0A%0ASELECT%20%3Fitem%20%3FitemLabel%0AWHERE%20%7B%0A%20%20%3Fpub_classes%20wdt%3AP2*%20wd%3AQ8%20.%0A%20%20%3Fitem%20wdt%3AP1%20%3Fpub_classes%20.%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20.%20%7D%0A%7D\",\n",
    "    output_format=\"dataframe\"\n",
    ")\n",
    "\n",
    "all_docs = pd.concat([pub_entities, report_entities]).reset_index(drop=True)\n",
    "all_docs['qid'] = all_docs['item'].apply(lambda x: x.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unidentified_pub_qids = all_docs[~all_docs['qid'].isin(geokb_pub_ids['qid'])]['qid'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 11568/16212 [15:05<05:48, 13.31it/s]"
     ]
    }
   ],
   "source": [
    "item_dump = Parallel(n_jobs=8, prefer=\"threads\")(delayed(entity_data)(i) for i in tqdm(unidentified_pub_qids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geokb_missing_pwid = all_docs[~all_docs['qid'].isin(geokb_pw_ids['qid'])].reset_index(drop=True)\n",
    "geokb_missing_pwid.head()\n",
    "# missing_qid_list = missing_pwid['qid'].to_list()\n",
    "# len(missing_qid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geokb_doi_indexid = pd.read_csv('./data/geokb_doi_indexid.csv')\n",
    "geokb_doi_indexid['qid'] = geokb_doi_indexid['item'].apply(lambda x: x.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_qids = geokb_doi_indexid[\n",
    "    (geokb_doi_indexid['indexId'].isnull())\n",
    "    &\n",
    "    (~geokb_doi_indexid['doi'].str.startswith('10.5066'))\n",
    "]['qid'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_check = Parallel(n_jobs=8, prefer=\"threads\")(delayed(get_pwid_values)(i, 'P114') for i in tqdm(check_qids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = []\n",
    "for l in claim_check:\n",
    "    if l is not None:\n",
    "        claims.extend(l)\n",
    "\n",
    "df_claims = pd.DataFrame(claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb.claim.get('Q144957')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Entities\n",
    "The process I'm using in building a big baseline of records like this is to pull in the different reference sources I need to use in linking new items into the GeoKB. For publications, this includes the type classification for instance of claims, USGS organizations that give us ownership and funding relationships based on metadata, place names for some of the categories of places in PW records, and the ORCID mapping to GeoKB items for contributors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USGS report types\n",
    "usgs_report_types = geokb.url_sparql_query(\n",
    "    sparql_url=\"https://geokb.wikibase.cloud/query/sparql?query=PREFIX%20wd%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fentity%2F%3E%0APREFIX%20wdt%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fprop%2Fdirect%2F%3E%0A%0ASELECT%20%3Fitem%20%3FitemLabel%0AWHERE%20%7B%0A%20%20%3Fitem%20wdt%3AP2%20wd%3AQ11%20.%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20.%20%7D%0A%7D\",\n",
    "    output_format=\"dataframe\"\n",
    ")\n",
    "\n",
    "usgs_report_types['instance_of_qid'] = usgs_report_types['item'].apply(lambda x: x.split('/')[-1])\n",
    "usgs_report_types['series_title'] = usgs_report_types['itemLabel'].apply(lambda x: x.replace('USGS ', ''))\n",
    "\n",
    "usgs_report_types.drop(columns=['item','itemLabel'], inplace=True)\n",
    "\n",
    "# USGS organizations\n",
    "usgs_orgs = geokb.url_sparql_query(\n",
    "    sparql_url=\"https://geokb.wikibase.cloud/query/sparql?query=PREFIX%20wd%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fentity%2F%3E%0APREFIX%20wdt%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fprop%2Fdirect%2F%3E%0A%0ASELECT%20%3Fitem%20%3FitemLabel%20%3Fitem_alt_label%20%3Finstance_ofLabel%0AWHERE%20%7B%0A%20%20%3Forg_types%20wdt%3AP2*%20wd%3AQ50862%20.%0A%20%20%3Fitem%20wdt%3AP1%20%3Forg_types%20.%0A%20%20%3Fitem%20wdt%3AP1%20%3Finstance_of%20.%0A%20%20OPTIONAL%20%7B%0A%20%20%20%20%3Fitem%20skos%3AaltLabel%20%3Fitem_alt_label%20.%0A%20%20%20%20FILTER%20(lang(%3Fitem_alt_label)%3D'en')%0A%20%20%7D%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20.%20%7D%0A%7D\",\n",
    "    output_format=\"dataframe\"\n",
    ")\n",
    "\n",
    "usgs_orgs['org_qid'] = usgs_orgs['item'].apply(lambda x: x.split('/')[-1])\n",
    "\n",
    "usgs_org_lookup = pd.concat([\n",
    "    usgs_orgs[['org_qid','itemLabel','instance_ofLabel']].drop_duplicates().rename(columns={'itemLabel': 'org_name'}),\n",
    "    usgs_orgs[['org_qid','item_alt_label','instance_ofLabel']].rename(columns={'item_alt_label': 'org_name'})\n",
    "])\n",
    "\n",
    "# Countries, states, and counties\n",
    "geokb_countries = geokb.url_sparql_query(\n",
    "    sparql_url=\"https://geokb.wikibase.cloud/query/sparql?query=PREFIX%20wdt%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fprop%2Fdirect%2F%3E%0A%0ASELECT%20%3Fitem%20%3FitemLabel%20%3Fitem_alt_label%20%3Fiso_country_code%0AWHERE%20%7B%0A%20%20%3Fitem%20wdt%3AP38%20%3Fiso_country_code%20.%0A%20%20OPTIONAL%20%7B%0A%20%20%20%20%3Fitem%20skos%3AaltLabel%20%3Fitem_alt_label%20.%0A%20%20%20%20FILTER%20(lang(%3Fitem_alt_label)%3D'en')%0A%20%20%7D%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20.%20%7D%0A%7D\",\n",
    "    output_format=\"dataframe\"\n",
    ")\n",
    "geokb_countries[\"object\"] = geokb_countries['item'].apply(lambda x: x.split('/')[-1])\n",
    "country_lookup = pd.concat([\n",
    "    geokb_countries[['object','itemLabel']].drop_duplicates().rename(columns={'itemLabel': 'place_name'}),\n",
    "    geokb_countries[['object','item_alt_label']].dropna().rename(columns={'item_alt_label': 'place_name'}),\n",
    "])\n",
    "country_lookup.drop_duplicates(inplace=True)\n",
    "\n",
    "geokb_states = geokb.url_sparql_query(\n",
    "    sparql_url=\"https://geokb.wikibase.cloud/query/sparql?query=PREFIX%20wdt%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fprop%2Fdirect%2F%3E%0A%0ASELECT%20%3Fitem%20%3FitemLabel%0AWHERE%20%7B%0A%20%20%3Fitem%20wdt%3AP12%20%3Fiso_code%20.%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20.%20%7D%0A%7D\",\n",
    "    output_format=\"dataframe\"\n",
    ")\n",
    "geokb_states[\"object\"] = geokb_states['item'].apply(lambda x: x.split('/')[-1])\n",
    "geokb_states.rename(columns={'itemLabel': 'place_name'}, inplace=True)\n",
    "geokb_states.drop(columns=\"item\", inplace=True)\n",
    "\n",
    "geokb_counties = geokb.url_sparql_query(\n",
    "    sparql_url=\"https://geokb.wikibase.cloud/query/sparql?query=PREFIX%20wd%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fentity%2F%3E%0APREFIX%20wdt%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fprop%2Fdirect%2F%3E%0A%0ASELECT%20%3Fitem%20%3FitemLabel%0AWHERE%20%7B%0A%20%20%3Fitem%20wdt%3AP1%20wd%3AQ481%20.%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20.%20%7D%0A%7D\",\n",
    "    output_format=\"dataframe\"\n",
    ")\n",
    "geokb_counties[\"object\"] = geokb_counties['item'].apply(lambda x: x.split('/')[-1])\n",
    "geokb_counties.rename(columns={'itemLabel': 'place_name'}, inplace=True)\n",
    "geokb_counties.drop(columns=\"item\", inplace=True)\n",
    "\n",
    "# PW type classifiers\n",
    "geokb_pub_classes = geokb.url_sparql_query(\n",
    "    sparql_url=\"https://geokb.wikibase.cloud/query/sparql?query=PREFIX%20wd%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fentity%2F%3E%0APREFIX%20wdt%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fprop%2Fdirect%2F%3E%0A%0ASELECT%20%3Fitem%20%3FitemLabel%20%3Fsubclass_of%20%3Fsubclass_ofLabel%0AWHERE%20%7B%0A%20%20%3Fitem%20wdt%3AP2*%20%3Ftop_classes%20.%0A%20%20VALUES%20%3Ftop_classes%20%7B%20wd%3AQ8%20wd%3AQ6%20%7D%0A%20%20%3Fitem%20wdt%3AP2%20%3Fsubclass_of%20.%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20.%20%7D%0A%7D\",\n",
    "    output_format=\"dataframe\"\n",
    ")\n",
    "\n",
    "geokb_pub_classes['instance_of_qid'] = geokb_pub_classes['item'].apply(lambda x: x.split(\"/\")[-1])\n",
    "geokb_pub_classes.drop(columns=\"item\", inplace=True)\n",
    "\n",
    "# Personnel ORCID IDs\n",
    "geokb_orcid = geokb.url_sparql_query(\n",
    "    sparql_url=\"https://geokb.wikibase.cloud/query/sparql?query=PREFIX%20wdt%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fprop%2Fdirect%2F%3E%0A%0ASELECT%20%3Fitem%20%3Forcid%0AWHERE%20%7B%0A%20%20%3Fitem%20wdt%3AP106%20%3Forcid%20.%0A%7D\",\n",
    "    output_format=\"dataframe\"\n",
    ")\n",
    "\n",
    "geokb_orcid['person_qid'] = geokb_orcid['item'].apply(lambda x: x.split(\"/\")[-1])\n",
    "geokb_orcid.drop(columns=['item'], inplace=True)\n",
    "geokb_orcid.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load from cache\n",
    "The initial baseline for the Pubs Warehouse catalog is somewhat large at 175K+ items. I ran an initial set of codes to pull all raw catalog records into a cache and then run some minimal processing to produce parquet files to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_core = pd.read_parquet('./data/pw_cache/pw_core.parquet')\n",
    "pw_contributors = pd.read_parquet('./data/pw_cache/pw_contributors.parquet')\n",
    "pw_texts = pd.read_parquet('./data/pw_texts.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_core_matching_titles = pw_core[pw_core['title'].isin(geokb_missing_pwid['itemLabel'])][['indexId','title']].rename(columns={'title': 'itemLabel'}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_core_matching_titles[pw_core_matching_titles.duplicated('itemLabel', keep=False)].sort_values('itemLabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redo_p114(geokb, qid, refs):\n",
    "    item = geokb.wbi.item.get(qid)\n",
    "    item_json = item.get_json()\n",
    "    if \"P114\" in item_json['claims']:\n",
    "        current_pwid = [i['mainsnak']['datavalue']['value'] for i in item_json['claims']['P114']]\n",
    "        item.claims.remove('P114')\n",
    "        response = item.write(summary=\"Removed P114 claim\")\n",
    "\n",
    "        item = geokb.wbi.item.get(qid)\n",
    "        new_claims = []\n",
    "        for pwid in current_pwid:\n",
    "            new_claims.append(\n",
    "                geokb.datatypes.ExternalID(\n",
    "                    prop_nr=\"P114\",\n",
    "                    value=pwid,\n",
    "                    references=refs\n",
    "                )\n",
    "            )\n",
    "        item.claims.add(new_claims)\n",
    "        response = item.write(summary=\"Re-added P114 claim\")\n",
    "        return response.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = geokb.models.References()\n",
    "refs.add(\n",
    "    geokb.datatypes.Item(\n",
    "        prop_nr=geokb.prop_lookup[\"data source\"],\n",
    "        value=\"Q54915\"\n",
    "    )\n",
    ")\n",
    "\n",
    "redo_p114(geokb, 'Q145467', refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redo_qids = geokb_missing_pwid[geokb_missing_pwid['itemLabel'].isin(pw_core['title'])]['qid'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwid_redo = Parallel(n_jobs=8, prefer=\"threads\")(delayed(redo_p114)(geokb, i, refs) for i in tqdm(redo_qids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_contributors_new = pw_contributors[~pw_contributors['indexId'].isin(geokb_index_ids)]\n",
    "pw_core_new = pw_core[\n",
    "    (~pw_core['indexId'].isin(geokb_index_ids)) \n",
    "    & \n",
    "    (~pw_core['doi'].isin(geokb_doi_ids))\n",
    "    &\n",
    "    (pw_core['indexId'].isin(pw_contributors_new['indexId']))\n",
    "]\n",
    "pw_texts_new = pw_texts[~pw_texts['indexId'].isin(geokb_index_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_index_id = geokb.url_sparql_query(\n",
    "    sparql_url=\"https://geokb.wikibase.cloud/query/sparql?query=PREFIX%20wd%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fentity%2F%3E%0APREFIX%20wdt%3A%20%3Chttps%3A%2F%2Fgeokb.wikibase.cloud%2Fprop%2Fdirect%2F%3E%0A%0ASELECT%20%3Fitem%20%3FitemLabel%0AWHERE%20%7B%0A%20%20%3Fpub_classes%20wdt%3AP2*%20wd%3AQ6%20.%0A%20%20%3Fitem%20wdt%3AP1%20%3Fpub_classes%20.%0A%20%20MINUS%20%7B%20%3Fitem%20wdt%3AP114%20%3FindexId%20.%20%7D%0A%20%20SERVICE%20wikibase%3Alabel%20%7B%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20.%20%7D%0A%7D\",\n",
    "    output_format=\"dataframe\"\n",
    ")\n",
    "missing_index_id['qid'] = missing_index_id['item'].apply(lambda x: x.split('/')[-1])\n",
    "missing_index_id.drop(columns=\"item\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_core_new_missing = pw_core_new[pw_core_new['title'].isin(missing_index_id['itemLabel'])].reset_index(drop=True)\n",
    "\n",
    "pw_core_new_missing_qid = pd.merge(\n",
    "    left=pw_core_new_missing,\n",
    "    right=missing_index_id.rename(columns={'itemLabel': 'title'}),\n",
    "    how=\"left\",\n",
    "    on=\"title\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_core_new_missing_qid[pw_core_new_missing_qid.duplicated(subset=\"qid\", keep=False)].sort_values('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usgs_org_links(x):\n",
    "    cost_center_orgs = [i.strip() for i in x['cost_centers']]\n",
    "    if isinstance(x['programNote'], str):\n",
    "        cost_center_orgs.append(x['programNote'])\n",
    "    return list(set(cost_center_orgs))\n",
    "\n",
    "def summary_text(row):\n",
    "    page_lines = [f\"= {row['title']} =\"]\n",
    "    if isinstance(row['docAbstract'], str):\n",
    "        page_lines.append(pypandoc.convert_text(row['docAbstract'], 'mediawiki', format='html'))\n",
    "    if isinstance(row['tableOfContents'], str):\n",
    "        page_lines.append(\"== Table of Contents ==\")\n",
    "        page_lines.append(pypandoc.convert_text(row['tableOfContents'], 'mediawiki', format='html'))\n",
    "\n",
    "    return '\\n'.join(page_lines)\n",
    "\n",
    "def split_place_names(x):\n",
    "    if not isinstance(x, str):\n",
    "        return None\n",
    "    delim = \",\"\n",
    "    if \";\" in x:\n",
    "        delim = \";\"\n",
    "    return [i.strip() for i in x.split(delim)]\n",
    "\n",
    "def county_state(row):\n",
    "    qualified_counties = []\n",
    "    if row['state'] and row['county']:\n",
    "        for st in row['state']:\n",
    "            for ct in row['county']:\n",
    "                qualified_counties.append(f\"{ct}, {st}\")\n",
    "    if qualified_counties:\n",
    "        return qualified_counties\n",
    "    return\n",
    "\n",
    "def pw_item_description(row):\n",
    "    descriptive_lines = []\n",
    "    if isinstance(row['pub_subtype'], str):\n",
    "        descriptive_lines.append(f\"a {row['pub_type']} ({row['pub_subtype']})\")\n",
    "    else:\n",
    "        descriptive_lines.append(f\"a {row['pub_type']}\")\n",
    "\n",
    "    if isinstance(row['publisher'], str):\n",
    "        descriptive_lines.append(f\"published by {row['publisher']}\")\n",
    "\n",
    "    if isinstance(row['series_title'], str):\n",
    "        descriptive_lines.append(f\"as part of series - {row['series_title']}\")\n",
    "\n",
    "    description_str = \" \".join(descriptive_lines)\n",
    "    if len(description_str) > 250:\n",
    "        description_str = f\"{description_str[:247]}...\"\n",
    "    return description_str\n",
    "\n",
    "def chunk_list(lst, chunk_size):\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i:i + chunk_size]\n",
    "        \n",
    "def search_orcid_list(orcid_list):\n",
    "    orcid_headers = {\n",
    "        \"Access token\": f\"Bearer {os.environ['ORCID_ACCESS_TOKEN']}\",\n",
    "        \"Content-type\": \"application/vnd.orcid+json\"\n",
    "    }\n",
    "\n",
    "    r = requests.get(\n",
    "        f\"https://pub.orcid.org/v3.0/expanded-search/?q=orcid:({' OR '.join(orcid_list)})\",\n",
    "        headers=orcid_headers\n",
    "    )\n",
    "    if r.status_code == 200:\n",
    "        if 'expanded-result' in r.json():\n",
    "            return pd.DataFrame(r.json()['expanded-result'])\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Items to Process\n",
    "\n",
    "Right now, I'm building a representation in the GeoKB for the following from the PW Catalog:\n",
    "* All USGS Numbered Series (because so many of the historic documents contain unique geoscientific information that we want to be able to work against)\n",
    "* All journal articles and other items who have ORCID-identified contributors (because of our capacity assessment use case)\n",
    "\n",
    "This leaves out some number of PW items we may want to revisit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_usgs_reports = pw_core[\n",
    "    (~pw_core['indexId'].isin(geokb_index_ids))\n",
    "    &\n",
    "    (pw_core['pub_subtype'] == 'USGS Numbered Series')\n",
    "].reset_index(drop=True)\n",
    "\n",
    "usgs_series_ref = geokb_pub_classes[geokb_pub_classes['subclass_ofLabel'] == 'USGS Numbered Series'].reset_index(drop=True)\n",
    "usgs_series_ref['series_title'] = usgs_series_ref['itemLabel'].str.replace('USGS ', '')\n",
    "\n",
    "missing_usgs_reports = pd.merge(\n",
    "    left=missing_usgs_reports,\n",
    "    right=usgs_series_ref[['instance_of_qid','series_title']],\n",
    "    how=\"left\",\n",
    "    on=\"series_title\"\n",
    ")\n",
    "\n",
    "# All other PW items\n",
    "pw_class_mapping = {\n",
    "    'Report': 'report',\n",
    "    'Article': 'scholarly article',\n",
    "    'Book chapter': 'conference paper',\n",
    "    'Conference Paper': 'conference paper',\n",
    "    'Book': 'book',\n",
    "    'Thesis': 'thesis',\n",
    "    'Newsletter': 'newsletter',\n",
    "    'Extramural-Authored Publication Paper': 'scholarly article'\n",
    "}\n",
    "\n",
    "df_pw_class_mapping = pd.DataFrame.from_dict(pw_class_mapping, orient='index', columns=['geokb_class_name'])\n",
    "df_pw_class_mapping.reset_index(inplace=True)\n",
    "df_pw_class_mapping.columns = ['pub_type', 'geokb_class_name']\n",
    "\n",
    "pw_other = pw_core[\n",
    "    (pw_core['indexId'].isin(pw_contributors['indexId']))\n",
    "    &\n",
    "    (pw_core['pub_subtype'] != 'USGS Numbered Series')\n",
    "    &\n",
    "    (~pw_core['indexId'].isin(geokb_index_ids))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "pw_other = pd.merge(\n",
    "    left=pw_other,\n",
    "    right=df_pw_class_mapping,\n",
    "    how=\"left\",\n",
    "    on=\"pub_type\"\n",
    ")\n",
    "\n",
    "other_pub_ref = geokb_pub_classes[geokb_pub_classes['itemLabel'].isin(pw_class_mapping.values())].reset_index(drop=True)\n",
    "\n",
    "pw_other = pd.merge(\n",
    "    left=pw_other,\n",
    "    right=other_pub_ref[['instance_of_qid','itemLabel']],\n",
    "    how=\"left\",\n",
    "    left_on=\"geokb_class_name\",\n",
    "    right_on=\"itemLabel\"\n",
    ")\n",
    "\n",
    "pw_other.drop(columns=['geokb_class_name','itemLabel'], inplace=True)\n",
    "\n",
    "# Combine the two for processing\n",
    "pw_core_combined = pd.concat([missing_usgs_reports, pw_other])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_pubs_on_ids = pw_core_combined[\n",
    "    (~pw_core_combined['doi'].isin(geokb_doi_items['doi']))\n",
    "    &\n",
    "    (~pw_core_combined['indexId'].isin(geokb_doi_items['indexId']))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_pubs_on_ids[missing_pubs_on_ids['title'].isin(geokb_doi_items['itemLabel'])][['indexId','doi','title','publicationYear']]['title'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing ORCIDs\n",
    "Pubs Warehouse records introduce a whole new set of person references who are not yet represented in the GeoKB, where our initial source of people was the USGS personnel profile pages. Most of these are non-USGS personnel who are co-authors on publications. Some of them are either current or former USGS personnel who do not have a personnel profile page, or at least not any more. ORCID does provide email addresses in some cases. In our processing, we record those that are usgs.gov addresses as a potential avenue to tie together additional information for people, including former staff in some cases. Depending on when people left the USGS, they may have a record retained in our internal directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_orcids = list(pw_contributors[\n",
    "    (pw_contributors['indexId'].isin(pw_core_combined['indexId']))\n",
    "    &\n",
    "    (~pw_contributors['orcid'].isin(geokb_orcid['orcid']))\n",
    "]['orcid'].unique())\n",
    "print(len(missing_orcids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcid_check_dfs = []\n",
    "for orcid_set in chunk_list(missing_orcids, 100):\n",
    "    check_orcid = search_orcid_list(orcid_set)\n",
    "    if check_orcid is not None:\n",
    "        orcid_check_dfs.append(check_orcid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvested_orcids = pd.concat(orcid_check_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvested_orcids = harvested_orcids.dropna(subset=[\"given-names\",\"family-names\",\"credit-name\"], how=\"all\")\n",
    "harvested_orcids['label'] = harvested_orcids.apply(lambda x: f\"{x['given-names'].split(',')[0]} {x['family-names']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harvested_orcids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The process of discovering new ORCIDs and determining what to do with them could be a separate operation. I'm leaving off doind anything with the remaining missing ORCID identifiers here. Most of these are cases where we already have a person with exactly the same name/label. The Wikibase API throws an exception on trying to write these records. A few of these are likely actual duplicate ORCID records, meaning that a person had more than one ORCID registered for one reason or another. Most of them likely need to be disambiguated at some point. It's not vital that we have all linkages captured, so we'll leave these off for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs_orcid = geokb.models.References()\n",
    "refs_orcid.add(\n",
    "    geokb.datatypes.Item(\n",
    "        prop_nr=geokb.prop_lookup['data source'],\n",
    "        value=\"Q138415\"\n",
    "    )\n",
    ")\n",
    "\n",
    "orcids_added = []\n",
    "\n",
    "for index, row in harvested_orcids.iterrows():\n",
    "    item = geokb.wbi.item.new()\n",
    "\n",
    "    item.labels.set('en', row['label'])\n",
    "    item.descriptions.set('en', 'a record for a person added from ORCID based on a contributor to a USGS publication')\n",
    "    if row['other-name']:\n",
    "        item.aliases.set('en', row['other-name'])\n",
    "\n",
    "    item.claims.add(\n",
    "        geokb.datatypes.Item(\n",
    "            prop_nr=geokb.prop_lookup['instance of'],\n",
    "            value=\"Q3\",\n",
    "            references=refs_orcid\n",
    "        )\n",
    "    )\n",
    "\n",
    "    item.claims.add(\n",
    "        geokb.datatypes.ExternalID(\n",
    "            prop_nr=geokb.prop_lookup['ORCID iD'],\n",
    "            value=row['orcid-id'],\n",
    "            references=refs_orcid\n",
    "        )\n",
    "    )\n",
    "\n",
    "    email_claims = []\n",
    "    for usgs_email in [i for i in row['email'] if i.endswith('usgs.gov')]:\n",
    "        email_claims.append(\n",
    "            geokb.datatypes.URL(\n",
    "                prop_nr=geokb.prop_lookup['email address'],\n",
    "                value=f\"mailto:{usgs_email.strip()}\",\n",
    "                references=refs_orcid\n",
    "            )\n",
    "        )\n",
    "    if email_claims:\n",
    "        item.claims.add(email_claims)\n",
    "\n",
    "    try:\n",
    "        response = item.write(\n",
    "            summary=\"Person record added from ORCID registry based on ORCID ID found as author/contributor to USGS publication\"\n",
    "        )\n",
    "        orcid_added = (row['orcid-id'],response.id)\n",
    "        orcids_added.append(orcid_added)\n",
    "        print(orcid_added)\n",
    "    except:\n",
    "        print(\"FAILURE:\", row['orcid-id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_core_combined['label'] = pw_core_combined['title'].apply(lambda x: x if len(x) <= 250 else f\"{x[:247]}...\")\n",
    "pw_core_combined['description'] = pw_core_combined.apply(pw_item_description, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USGS Organization Mapping\n",
    "We have a combination of a list of \"USGS Cost Centers\" and a separate field called programNote that contain links between publications and USGS organizations. Both of these contain some names that can be reasonably matched to our best master representation of USGS organizational units. In preparation, I did introduce a number of aliases into the GeoKB for the slight deviation on names used in the PW Catalog. We ultimately need to get this information better nailed down with some type of persistent, resolvable identifier for organizations (e.g., ROR should work in most cases).\n",
    "\n",
    "I use the classification of organizations introduced in the GeoKB to distinguish between USGS Programs that fund the work, putting these into a funder linkage, and the other organizational types that I loosely claimed as \"owner\" of the products (meaning that the organizational unit was and is logically responsible for the product).\n",
    "\n",
    "Given the fuzziness of the text values used in the PW Catalog (e.g., obvious misspellings, etc.), the information on cost centers/programs is a little bit suspect. However, I'm accepting it at face value for the time being.\n",
    "\n",
    "The other piece of information we need to revisit here is the organizational affiliation for contributors. Presumably, this is the affiliation at the time of the specific item being published (or at least submitted for publication). We could potentially use this, at least from first authors, as a further indication of the organizational connection for the publication itself. For now, I'm leaving that relationship alone. We can also potentially use contributor affiliations in pub records for the people themselves, adding additional affiliation claims qualified by publication dates. However, given the questions about just how good or accurate this information is, I'm leaving that alone for now as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_mapping_props = [\n",
    "    \"indexId\",\n",
    "    \"cost_centers\",\n",
    "    \"programNote\"\n",
    "]\n",
    "\n",
    "pw_org_links = pw_core_combined[org_mapping_props].reset_index(drop=True)\n",
    "\n",
    "pw_org_links['org_name'] = pw_org_links.apply(usgs_org_links, axis=1)\n",
    "\n",
    "pw_org_mapping = pd.merge(\n",
    "    left=pw_org_links[pw_org_links['org_name'].str.len() > 0][['indexId','org_name']].explode('org_name'),\n",
    "    right=usgs_org_lookup[usgs_org_lookup['org_qid'] != 'Q44210'],\n",
    "    how=\"inner\",\n",
    "    on=\"org_name\"\n",
    ")\n",
    "pw_org_mapping['predicate'] = pw_org_mapping['instance_ofLabel'].apply(lambda x: geokb.prop_lookup['funder'] if x == \"USGS Program\" else geokb.prop_lookup['owner'])\n",
    "\n",
    "pw_org_mapping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_contributions = pd.merge(\n",
    "    left=pw_contributors[pw_contributors['indexId'].isin(pw_core_combined['indexId'])][['indexId','orcid','pub_role']],\n",
    "    right=geokb_orcid,\n",
    "    how=\"inner\",\n",
    "    on=\"orcid\"\n",
    ")\n",
    "pw_contributions['predicate'] = pw_contributions['pub_role'].apply(lambda x: geokb.prop_lookup[x[:-1]])\n",
    "\n",
    "pw_contributions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geo Names\n",
    "The PW Catalog stores geo references in a couple of different ways. There are place names in country, state, county, city, and otherGeospatial fields. There is also a structure containing FeatureCollection GeoJSON. Both of these are somewhat suspect in terms of data quality when we find so many obvious misspellings in place names. For now, I decided to use country, state, and county references. I deal with the vagary of having counties listed without their states by putting together county and state myself from available information and only accepting cases where I get an exact match on name to how we represented these entities in the GeoKB (e.g., \"Mesa County, Colorado\").\n",
    "\n",
    "The otherGeospatial property contains what are likely more interesting place names that we'll want to deal with as we further flesh out place name references in the GeoKB. The actual geometry in the feature collections might also be quite interesting, but there does appear to be a mishmash of very specific footprints, perhaps providing the study site from a given paper, and things that are more likely boundaries of entities that may as well be names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_geo_props = [\n",
    "    \"indexId\",\n",
    "    \"country\",\n",
    "    \"state\",\n",
    "    \"county\"\n",
    "]\n",
    "\n",
    "pw_geonames = pw_core_combined[pw_geo_props].reset_index(drop=True)\n",
    "pw_geonames['country'] = pw_geonames['country'].apply(split_place_names)\n",
    "pw_geonames['state'] = pw_geonames['state'].apply(split_place_names)\n",
    "pw_geonames['county'] = pw_geonames['county'].apply(split_place_names)\n",
    "pw_geonames['county'] = pw_geonames.apply(county_state, axis=1)\n",
    "\n",
    "place_name_dfs = []\n",
    "for place_type in [i for i in pw_geo_props if i != \"indexId\"]:\n",
    "    df = pw_geonames[['indexId',place_type]].explode(place_type).dropna()\n",
    "    df['name_type'] = place_type\n",
    "    place_name_dfs.append(df.rename(columns={place_type: 'place_name'})[['indexId','name_type','place_name']])\n",
    "\n",
    "pw_place_names = pd.concat(place_name_dfs)\n",
    "\n",
    "pw_place_name_claims = pd.concat([\n",
    "    pd.merge(\n",
    "        left=pw_place_names[pw_place_names['name_type'] == 'country'],\n",
    "        right=country_lookup,\n",
    "        how=\"inner\",\n",
    "        on=\"place_name\"\n",
    "    ),\n",
    "    pd.merge(\n",
    "        left=pw_place_names[pw_place_names['name_type'] == 'state'],\n",
    "        right=geokb_states,\n",
    "        how=\"inner\",\n",
    "        on=\"place_name\"\n",
    "    ),\n",
    "    pd.merge(\n",
    "        left=pw_place_names[pw_place_names['name_type'] == 'county'],\n",
    "        right=geokb_counties,\n",
    "        how=\"inner\",\n",
    "        on=\"place_name\"\n",
    "    )\n",
    "])\n",
    "pw_place_name_claims['predicate'] = geokb.prop_lookup['addresses place']\n",
    "\n",
    "pw_place_name_claims.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive texts\n",
    "It is potentially quite useful to introduce the larger text \"blobs\" we have for some PW Catalog items into the GeoKB itself. We take advantage of the wiki page functionality of \"item discussions\" to store these texts in wiki markup. This makes the text available as part of Wikibase/Mediawiki search indexing, which may have its own benefits for UI and API uses. It also puts the texts somewhere resolvable so that we can reference them effectively in provenance for anything we derive through text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_text_props = [\n",
    "    \"indexId\",\n",
    "    \"title\",\n",
    "    \"docAbstract\",\n",
    "    \"tableOfContents\"\n",
    "]\n",
    "\n",
    "pw_text_content = pw_core_combined[summary_text_props].reset_index(drop=True)\n",
    "\n",
    "pw_text_content = pw_text_content[\n",
    "    (pw_text_content['docAbstract'].notnull())\n",
    "    |\n",
    "    (pw_text_content['tableOfContents'].notnull())\n",
    "].reset_index(drop=True)\n",
    "\n",
    "pw_text_content['docAbstract'] = pw_text_content['docAbstract'].apply(lambda x: x if isinstance(x, str) and len(x) > 75 else pd.NA)\n",
    "pw_text_content.dropna(subset=['docAbstract','tableOfContents'], how=\"all\", inplace=True)\n",
    "\n",
    "# The process of converting HTML to wikimedia markup takes a while\n",
    "pw_text_content['summary_text'] = pw_text_content.apply(summary_text, axis=1)\n",
    "pw_text_content = pw_text_content[['indexId','summary_text']].reset_index(drop=True)\n",
    "\n",
    "pw_text_content.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to GeoKB\n",
    "With pre-processing done, we can now build new product items in the GeoKB with all of the following:\n",
    "* labels (titles) and descriptions\n",
    "* classification into one of the publication/report categories\n",
    "* publication year\n",
    "* author/editor/compiler contributor links\n",
    "* organization links (funder or owner)\n",
    "* place name links\n",
    "* summary text on item discussion wiki pages (abstract and/or toc)\n",
    "\n",
    "Before committing, I looked to see if we had duplicate labels. It turns out there are quite a few, many of which appear to be likely duplicates in the PW Catalog itself. They have separate indexId values, but other aspects of the record do look the same. It's hard to say if these are cataloging mistakes or what the issue is. To deal with it, I go ahead and drop duplicates, keeping any that have DOI values as the most valuable for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_items = pw_core_combined[['indexId','doi','publicationYear','label','description','instance_of_qid','pub_subtype']].reset_index(drop=True)\n",
    "pw_items_sorted = pw_items.sort_values('doi')\n",
    "pw_unique_items = pw_items_sorted.drop_duplicates(subset='label', keep='first')\n",
    "pw_unique_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_of_claims = pw_unique_items[['indexId','instance_of_qid']].reset_index(drop=True).rename(columns={'indexId': 'subject', 'instance_of_qid': 'object'})\n",
    "instance_of_claims['predicate'] = 'P1'\n",
    "\n",
    "claims = pd.concat([\n",
    "    instance_of_claims[['subject','predicate','object']],\n",
    "    pw_contributions[pw_contributions['indexId'].isin(pw_unique_items['indexId'])][['indexId','predicate','person_qid']].rename(columns={'indexId': 'subject', 'person_qid': 'object'}),\n",
    "    pw_org_mapping[pw_org_mapping['indexId'].isin(pw_unique_items['indexId'])][['indexId','predicate','org_qid']].rename(columns={'indexId': 'subject', 'org_qid': 'object'}),\n",
    "    pw_place_name_claims[pw_place_name_claims['indexId'].isin(pw_unique_items['indexId'])][['indexId','predicate','object']].rename(columns={'indexId': 'subject'})\n",
    "])\n",
    "\n",
    "grouped_claims = claims.groupby([\"subject\",\"predicate\"])['object'].agg(list).reset_index()\n",
    "\n",
    "grouped_claims.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_text_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = geokb.models.References()\n",
    "refs.add(\n",
    "    geokb.datatypes.Item(\n",
    "        prop_nr=geokb.prop_lookup[\"data source\"],\n",
    "        value=\"Q54915\"\n",
    "    )\n",
    ")\n",
    "\n",
    "pw_items = []\n",
    "failed_items = []\n",
    "\n",
    "for index, row in commit_items.iterrows():\n",
    "    item_claims = grouped_claims[grouped_claims['subject'] == row['indexId']]\n",
    "    summary_text = pw_text_content[pw_text_content['indexId'] == row['indexId']]\n",
    "\n",
    "    item = geokb.wbi.item.get(row['qid'])\n",
    "\n",
    "    item.labels.set('en', row['label'])\n",
    "    item.descriptions.set('en', row['description'])\n",
    "    \n",
    "    if row['pub_subtype'] == \"USGS Numbered Series\":\n",
    "        item.aliases.set('en', row['indexId'])\n",
    "\n",
    "    for idx, claim_group in item_claims.iterrows():\n",
    "        claims = []\n",
    "        for obj_qid in claim_group['object']:\n",
    "            claims.append(\n",
    "                geokb.datatypes.Item(\n",
    "                    prop_nr=claim_group['predicate'],\n",
    "                    value=obj_qid,\n",
    "                    references=refs\n",
    "                )\n",
    "            )\n",
    "        item.claims.add(claims)\n",
    "\n",
    "    if isinstance(row['doi'], str):\n",
    "        item.claims.add(\n",
    "            geokb.datatypes.ExternalID(\n",
    "                prop_nr=geokb.prop_lookup['DOI'],\n",
    "                value=row['doi'],\n",
    "                references=refs\n",
    "            )\n",
    "        )\n",
    "\n",
    "    item.claims.add(\n",
    "        geokb.datatypes.ExternalID(\n",
    "            prop_nr=geokb.prop_lookup['USGS Publications Warehouse IndexID'],\n",
    "            value=row['indexId'],\n",
    "            references=refs\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if isinstance(row['publicationYear'], str):\n",
    "        item.claims.add(\n",
    "            geokb.datatypes.Time(\n",
    "                prop_nr=geokb.prop_lookup['publication date'],\n",
    "                time=f\"+{row['publicationYear']}-01-01T00:00:00Z\",\n",
    "                precision=9,\n",
    "                references=refs\n",
    "            )\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        response = item.write(\n",
    "            summary=\"Updated publication item from USGS Pubs Warehouse\"\n",
    "        )\n",
    "        item_tuple = (row['indexId'], response.id)\n",
    "        pw_items.append(item_tuple)\n",
    "        print(item_tuple)\n",
    "\n",
    "        if not summary_text.empty:\n",
    "            page = geokb.mw_site.pages[f\"Item_talk:{response.id}\"]\n",
    "            response_summary = page.edit(\n",
    "                summary_text.iloc[0]['summary_text'],\n",
    "                summary=\"Added abstract and other texts to publication item's discussion page for reference\"\n",
    "            )\n",
    "            print(response_summary['title'])\n",
    "    except Exception as e:\n",
    "        failure_packet = (row['indexId'], str(e))\n",
    "        failed_items.append(failure_packet)\n",
    "        print(failure_packet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpub Relationships\n",
    "I need to work through the relationships once I have most catalog items represented in the GeoKB. This will established part of and superseded by relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pub Links\n",
    "I need to decide what all to do with the links to content. We may want to selectively store links in the knowledge graph context that will ultimately serve as source material for some type of processing that results in new claims. This will aid in a better provenance trace on those claims without having to navigate another system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geokb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
