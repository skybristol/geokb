{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook handles the process of scraping through the USGS staff profiles paginated interface to pull together a current inventory in structured data and dropping that to the Mediawiki discussion page for the item representing that source in the GeoKB. The inventory is then processed with a separate algorithm to deal with the content.\n",
    "\n",
    "After figuring out there's a limit on the size of data I can push, I stripped this process way back to simply scrape all of the unique profile URLs (and really just the name part of the profile URL). I really only need these pointers over time to figure out what the GeoKB already knows about and what profiles need to be pulled and added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import mwclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I could get these dynamically, but they are the essential variables we need to run this\n",
    "source_item = \"Q44323\"\n",
    "profile_inventory_url = \"https://www.usgs.gov/connect/staff-profiles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to get the range of pages to scrape\n",
    "def last_page():\n",
    "    r_profile_inventory = requests.get(profile_inventory_url)\n",
    "    soup_profile_inventory = BeautifulSoup(r_profile_inventory.content, 'html.parser')\n",
    "    last_page_link = soup_profile_inventory.find('a', {'title': 'Go to last page'})['href']\n",
    "    if not last_page_link:\n",
    "        return\n",
    "\n",
    "    last_page_url = \"\".join([profile_inventory_url, last_page_link])\n",
    "    parsed_url = urlparse(last_page_url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    last_page_num = query_params.get(\"page\")\n",
    "    if last_page_num:\n",
    "        return int(last_page_num[0])\n",
    "\n",
    "# Scrape the basic profile name string from links on the page\n",
    "def profiles_from_inventory_page(page_num):\n",
    "    url = \"?\".join([profile_inventory_url, f\"page={str(page_num)}\"])\n",
    "    r_inventory = requests.get(url)\n",
    "    if r_inventory.status_code == 200:\n",
    "        soup = BeautifulSoup(r_inventory.content, 'html.parser')\n",
    "        container = soup.find('div', {'class': 'views-element-container'})\n",
    "        return [l['href'].split('/')[-1] for l in container.find_all('a', href=lambda href: href.startswith('/staff-profiles/') if href else False)]\n",
    "\n",
    "# Combine all lists returned in parallel processing into one with unique values\n",
    "def inventory_list(inventories):\n",
    "    inventory_records = []\n",
    "    for i in inventories:\n",
    "        inventory_records.extend(i)\n",
    "\n",
    "    return list(set(inventory_records))\n",
    "\n",
    "# Create a secure connection to the Wikibase so we can write to it\n",
    "def create_authenticated_site(user_name, password):\n",
    "    site = mwclient.Site('geokb.wikibase.cloud', path='/w/', scheme='https')\n",
    "    site.login(user_name, password)\n",
    "\n",
    "    return site\n",
    "\n",
    "# Write out the inventory list to the Wikibase\n",
    "def write_inventory(qid, site, profile_list):\n",
    "    inventory_cache_page = f\"Item_talk:{qid}\"\n",
    "    \n",
    "    try:\n",
    "        page = site.pages[inventory_cache_page]\n",
    "        page.save(','.join(profile_list), summary=f'Added cache of USGS staff profile inventory')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 490/490 [00:56<00:00,  8.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the last page of the inventory\n",
    "last_page_num = last_page()\n",
    "\n",
    "# Scrape the inventory pages as fast as possible\n",
    "inventories = Parallel(n_jobs=-1, prefer='threads')(delayed(profiles_from_inventory_page)(i) for i in tqdm(range(last_page_num+1)))\n",
    "\n",
    "# Put inventories together into dataframe\n",
    "inventory_list = inventory_list(inventories)\n",
    "\n",
    "# Establish GeoKB Wikibase site connection\n",
    "mw_site = create_authenticated_site(os.environ['WB_BOT_GEOKB_CLOUD'], os.environ['WB_BOT_PASS_GEOKB_CLOUD'])   \n",
    "\n",
    "# Write the inventory list to the source page\n",
    "write_inventory(source_item, mw_site, inventory_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geokb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
